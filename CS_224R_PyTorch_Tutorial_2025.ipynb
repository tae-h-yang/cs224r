{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tae-h-yang/cs224r/blob/main/CS_224R_PyTorch_Tutorial_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlZalm-n6RRp"
      },
      "source": [
        "*Author*: Marcel Torne\n",
        "\n",
        "With inspiration from the previous CS224r tutorial by : Kyle Hsu with revisions from: Rafael Rafailov, Evan Liu, Fahim Tajwar, Annie Xie\n",
        "\n",
        "And from the CS224n tutorial by: Dilara Soylu and Ethan Chi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgJxPU226RRq"
      },
      "source": [
        "Some installation requirements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "7BFpSiM16RRq",
        "outputId": "fb76dbcf-1e4f-4f0b-d618-e2b130ecf73d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipycanvas in /usr/local/lib/python3.11/dist-packages (0.13.3)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.11/dist-packages (from ipycanvas) (7.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ipycanvas) (2.0.2)\n",
            "Requirement already satisfied: pillow>=6.0 in /usr/local/lib/python3.11/dist-packages (from ipycanvas) (11.1.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas) (3.0.13)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipycanvas) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipycanvas) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipycanvas) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipycanvas) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipycanvas) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipycanvas) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipycanvas) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipycanvas) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.6.0->ipycanvas) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.6.0->ipycanvas) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.6.0->ipycanvas) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.6.0->ipycanvas) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets<9,>=7.6.0->ipycanvas) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipycanvas) (5.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipycanvas) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (1.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipycanvas) (4.3.7)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipycanvas) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (0.24.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (4.13.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipycanvas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzLQ1tyf6RRr"
      },
      "source": [
        "# Introduction\n",
        "Welcome to the PyTorch tutorial for CS 224R! This colab notebook accompanies [these slides](https://docs.google.com/presentation/d/1MC2XqjzN0M-R-Y-uMjefAqqEZRg7q2igFqAfu1P2_yo/edit?usp=sharing). If you haven't already, enable a GPU for this colab instance by doing \"Edit\" -> \"Notebook settings\" -> \"Hardware accelerator\" drop-down -> \"GPU\" -> \"Save\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f49lq3Qk6RRr"
      },
      "source": [
        "Let's make sure we're using the right Python and PyTorch versions, and that we have a GPU at our disposal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "T2Y_yHW86RRr",
        "outputId": "44569d31-a3a0-47c3-89d4-30e37f291101",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version info: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "PyTorch version info: 2.6.0+cu124\n",
            "PyTorch detects a GPU: True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "print(f'Python version info: {sys.version}')\n",
        "print(f'PyTorch version info: {torch.__version__}')\n",
        "print(f'PyTorch detects a GPU: {torch.cuda.is_available()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_SD9tCD6RRs"
      },
      "source": [
        "# Overview\n",
        "We'll begin looking at the very fundamentals of PyTorch, how to create and operate with tensors, and how to leverage your GPU to do your operations faster. Then we will go through the basics of PyTorch by looking into training an agent to train the custom made game for the class, the Stanford Flappybird.\n",
        "\n",
        "I also added some additional details that we might go through if there's time or that you can go through on your own. These details include understanding better autograd by solving and ODE in PyTorch.\n",
        "\n",
        "I hope all this information will be useful for your homeworks for this course but also for your professional career."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEnIL6zF6RRs"
      },
      "source": [
        "# Pytorch fundamentals\n",
        "\n",
        "**Tensors** are PyTorch's most basic building block. Each tensor is a multi-dimensional matrix. The PyTorch library API is very similar to the numpy library.\n",
        "\n",
        "Let's look into how to create a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oF2Ezlt06RRs",
        "outputId": "9430ab24-2d8e-4ee8-85c9-1dc87fc5531f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor from list of lists:\n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "Tensor from numpy: \n",
            " tensor([[1., 2.],\n",
            "        [3., 4.]], dtype=torch.float64)\n",
            "\n",
            "Random tensor:\n",
            " tensor([[0.3009, 1.4527, 0.4828],\n",
            "        [0.1567, 0.4523, 1.2842]])\n",
            "\n",
            "Zeros tensor:\n",
            " tensor([[0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# 1. Create a tensor from a list of lists\n",
        "tensor_from_list = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"Tensor from list of lists:\\n\", tensor_from_list)\n",
        "\n",
        "# 2. Create a tensor from a numpy array\n",
        "np_array = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "tensor_from_numpy = torch.from_numpy(np_array)\n",
        "print(\"\\nTensor from numpy: \\n\", tensor_from_numpy)\n",
        "\n",
        "# 3. Create a tensor using the PyTorch library\n",
        "random_tensor = torch.randn(2, 3)  # 2x3 tensor with random values from a normal distribution\n",
        "zeros_tensor = torch.zeros(2, 6) # 2x6 tensor with zeros\n",
        "print(\"\\nRandom tensor:\\n\", random_tensor)\n",
        "print(\"\\nZeros tensor:\\n\", zeros_tensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUHLyTBd6RRs"
      },
      "source": [
        "Use `.shape` to inspect the size of a tensor! This will be incredibly useful to debug your code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qlQMdjNo6RRs",
        "outputId": "9e944d75-4be6-41e1-f9c3-10c7328d3bde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor shape:  torch.Size([2, 6])\n"
          ]
        }
      ],
      "source": [
        "print(\"Tensor shape: \", zeros_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOWT-rcG6RRs"
      },
      "source": [
        "Accessing elements of the tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZdfUMecx6RRs",
        "outputId": "ba7db8c6-cd4c-4ade-bacf-13cd95f984a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor:\n",
            " tensor([[ 0,  1,  2,  3,  4,  5],\n",
            "        [ 6,  7,  8,  9, 10, 11]])\n",
            "\n",
            "Access all first elements in second dimension:\n",
            " tensor([0, 6])\n"
          ]
        }
      ],
      "source": [
        "ordered_tensor = torch.arange(12).reshape(2, 6)\n",
        "\n",
        "print(\"Tensor:\\n\", ordered_tensor)\n",
        "print(\"\\nAccess all first elements in second dimension:\\n\", ordered_tensor[:,0] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD5HFvmR6RRs"
      },
      "source": [
        "Operations with tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Dzmg5oQb6RRs",
        "outputId": "92260491-fb61-482b-8f6c-30184670f500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor:\n",
            " tensor([0., 0., 0., 0., 0.])\n",
            "Modified tensor at indexes 1 and 3:\n",
            " tensor([ 0., 10.,  0., 20.,  0.])\n"
          ]
        }
      ],
      "source": [
        "# Modify just the tensor at certain indexes\n",
        "tensor = torch.zeros(5)\n",
        "print(\"Original tensor:\\n\", tensor)\n",
        "tensor[[1, 3]] = torch.tensor([10.0, 20.0])\n",
        "print(\"Modified tensor at indexes 1 and 3:\\n\", tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "n-bdquBH6RRs",
        "outputId": "148723f0-0f3a-49eb-a766-522cf213c01d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original tensor:\n",
            " tensor([1., 2., 3.])\n",
            "Tensor multiplied by scalar 5:\n",
            " tensor([ 5., 10., 15.])\n"
          ]
        }
      ],
      "source": [
        "# Multiply tensors by a scalar\n",
        "tensor = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(\"\\nOriginal tensor:\\n\", tensor)\n",
        "scaled_tensor = tensor * 5\n",
        "print(\"Tensor multiplied by scalar 5:\\n\", scaled_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UbEmMGGW6RRs",
        "outputId": "52d5a2f5-4c77-4956-f5d5-f9cd9da7e961",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Matrix A:\n",
            " tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "Matrix B:\n",
            " tensor([[5, 6],\n",
            "        [7, 8]])\n",
            "Matrix multiplication (A @ B):\n",
            " tensor([[19, 22],\n",
            "        [43, 50]])\n"
          ]
        }
      ],
      "source": [
        "# Matrix-multiplication\n",
        "A = torch.tensor([[1, 2], [3, 4]])\n",
        "B = torch.tensor([[5, 6], [7, 8]])\n",
        "print(\"\\nMatrix A:\\n\", A)\n",
        "print(\"Matrix B:\\n\", B)\n",
        "matmul_result = torch.matmul(A, B)\n",
        "print(\"Matrix multiplication (A @ B):\\n\", matmul_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "o6zFBEcH6RRt",
        "outputId": "f7f2e64b-8083-4fa2-c1ef-aa5f951008b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tensor t1:\n",
            " tensor([[1, 2]])\n",
            "Tensor t2:\n",
            " tensor([[3, 4]])\n",
            "Concatenation along dim=0 (vertical stack):\n",
            " tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "Concatenation along dim=1 (horizontal stack):\n",
            " tensor([[1, 2, 3, 4]])\n"
          ]
        }
      ],
      "source": [
        "# Concatenation of tensors\n",
        "t1 = torch.tensor([[1, 2]])\n",
        "t2 = torch.tensor([[3, 4]])\n",
        "print(\"\\nTensor t1:\\n\", t1)\n",
        "print(\"Tensor t2:\\n\", t2)\n",
        "concat_dim0 = torch.cat([t1, t2], dim=0)\n",
        "concat_dim1 = torch.cat([t1, t2], dim=1)\n",
        "print(\"Concatenation along dim=0 (vertical stack):\\n\", concat_dim0)\n",
        "print(\"Concatenation along dim=1 (horizontal stack):\\n\", concat_dim1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "wHh1WbhY6RRt",
        "outputId": "bb60a461-11d6-412c-a54d-66fa660a84d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tensor a:\n",
            " tensor([1., 2., 0.])\n",
            "torch.min of a single tensor (returns the minimum value in the tensor):\n",
            " tensor(0.)\n",
            "\n",
            "Tensor a:\n",
            " tensor([1., 2., 0.])\n",
            "Tensor b:\n",
            " tensor([0., 3., 1.])\n",
            "torch.minimum between two tensors (element-wise min):\n",
            " tensor([0., 2., 0.])\n"
          ]
        }
      ],
      "source": [
        "# torch.min vs torch.minimum\n",
        "a = torch.tensor([1.0, 2.0, 0])\n",
        "b = torch.tensor([0.0, 3.0, 1.0])\n",
        "print(\"\\nTensor a:\\n\", a)\n",
        "min_result = torch.min(a)\n",
        "print(\"torch.min of a single tensor (returns the minimum value in the tensor):\\n\", min_result)\n",
        "\n",
        "print(\"\\nTensor a:\\n\", a)\n",
        "print(\"Tensor b:\\n\", b)\n",
        "minimum_result = torch.minimum(a, b)\n",
        "print(\"torch.minimum between two tensors (element-wise min):\\n\", minimum_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKT0w2Gn6RRt"
      },
      "source": [
        "Be careful: tensors are passed as reference, which means that any modifications made to a tensor will be repercuted to all of the instances! Make sure to `.clone()` your tensor if you don't want that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FvC4Binq6RRt",
        "outputId": "38c82096-c4b8-4a3a-cfb1-95e4d21ebd55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without clone:\n",
            "Tensor a (after modifying b): tensor([99.,  2.,  3.])\n",
            "Tensor b: tensor([99.,  2.,  3.])\n",
            "\n",
            "With clone:\n",
            "Tensor a (unchanged): tensor([1., 2., 3.])\n",
            "Tensor b (modified): tensor([99.,  2.,  3.])\n"
          ]
        }
      ],
      "source": [
        "# Without clone: modifying one tensor affects the other\n",
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = a  # b is just another reference to a\n",
        "b[0] = 99.0\n",
        "\n",
        "print(\"Without clone:\")\n",
        "print(\"Tensor a (after modifying b):\", a)\n",
        "print(\"Tensor b:\", b)\n",
        "\n",
        "# With clone: tensors are independent\n",
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = a.clone()  # b is a deep copy of a\n",
        "b[0] = 99.0\n",
        "\n",
        "print(\"\\nWith clone:\")\n",
        "print(\"Tensor a (unchanged):\", a)\n",
        "print(\"Tensor b (modified):\", b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA59mSaw6RRt"
      },
      "source": [
        "Transfer your tensors to the GPU for faster operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ntd6oMNg6RRt",
        "outputId": "ba49038b-7c22-4d17-e80b-3120c0a7976f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "\n",
            "Time on CPU: 26.3363 seconds\n",
            "Time on GPU: 1.0260 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Set tensor size\n",
        "size = 10000\n",
        "\n",
        "# Ensure CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# --- CPU computation ---\n",
        "a_cpu = torch.randn(size, size)\n",
        "b_cpu = torch.randn(size, size)\n",
        "\n",
        "start = time.time()\n",
        "c_cpu = torch.matmul(a_cpu, b_cpu)\n",
        "end = time.time()\n",
        "print(\"\\nTime on CPU: {:.4f} seconds\".format(end - start))\n",
        "\n",
        "# --- GPU computation ---\n",
        "a_gpu = a_cpu.to(\"cuda\")\n",
        "b_gpu = b_cpu.to(\"cuda\")\n",
        "\n",
        "# Warm-up CUDA (first operation can be slower due to lazy init)\n",
        "_ = torch.matmul(a_gpu, b_gpu)\n",
        "\n",
        "start = time.time()\n",
        "c_gpu = torch.matmul(a_gpu, b_gpu)\n",
        "assert c_gpu[0,0] != a_gpu[0,0] # just waiting for the operation to finish\n",
        "end = time.time()\n",
        "print(\"Time on GPU: {:.4f} seconds\".format(end - start))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsbpvr016RRt"
      },
      "source": [
        "#PyTorch Basics via a Simple Imitation Learning Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP0lSnIa6RRt"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8gJapmR6RRt"
      },
      "source": [
        "The basic components for running imitation learning are usually to have an environment, a dataset, a policy and a training loop. We will focus on the two last."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7hakN5U6RRt"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from ipycanvas import MultiCanvas\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "import time\n",
        "import requests\n",
        "from PIL import Image as PILImage\n",
        "from ipywidgets import Image as WidgetImage\n",
        "import io\n",
        "\n",
        "def load_widget_image_from_url(url, resize_to=(40, 200)):\n",
        "    response = requests.get(url)\n",
        "    img = PILImage.open(io.BytesIO(response.content)).convert(\"RGBA\").resize(resize_to)\n",
        "    buffer = io.BytesIO()\n",
        "    img.save(buffer, format=\"PNG\")\n",
        "    return WidgetImage(value=buffer.getvalue(), format='png')\n",
        "\n",
        "class FlappyMazeCanvasEnv:\n",
        "    def __init__(self, background_img=None, pipe_img=None, bird_up_img=None, bird_down_img=None,\n",
        "                 width=600, height=300, pipe_gap=60, pipe_width=30,\n",
        "                 min_pipe_spacing=300, max_pipe_spacing=600, agent_radius=10,\n",
        "                 pipe_speed=20, frame_rate=10):\n",
        "\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.pipe_gap = pipe_gap\n",
        "        self.pipe_width = pipe_width\n",
        "        self.pipe_speed = pipe_speed\n",
        "        self.min_pipe_spacing = min_pipe_spacing\n",
        "        self.max_pipe_spacing = max_pipe_spacing\n",
        "        self.agent_radius = agent_radius\n",
        "        self.frame_rate = frame_rate\n",
        "\n",
        "        self.canvas = MultiCanvas(3, width=self.width, height=self.height)\n",
        "        self.bg_layer, self.pipe_layer, self.agent_layer = self.canvas\n",
        "\n",
        "        self.background = load_widget_image_from_url(\"https://raw.githubusercontent.com/MarcelTorne/Stanford224rTutorial/main/background2.png\", (600, 300))\n",
        "        self.pipe = load_widget_image_from_url(\"https://raw.githubusercontent.com/MarcelTorne/Stanford224rTutorial/main/pipe.png\", (100, 200))\n",
        "        self.bird_down = load_widget_image_from_url(\"https://raw.githubusercontent.com/MarcelTorne/Stanford224rTutorial/main/robobird_down.png\", (60, 60))\n",
        "        self.bird_up = load_widget_image_from_url(\"https://raw.githubusercontent.com/MarcelTorne/Stanford224rTutorial/main/robobird_up.png\", (60, 60))\n",
        "\n",
        "\n",
        "        self.agent_x = 100\n",
        "        self.gravity = 1.5\n",
        "        self.lift = -8.0\n",
        "\n",
        "        from IPython.display import display\n",
        "        display(self.canvas)\n",
        "\n",
        "        if self.background is not None:\n",
        "            self.bg_layer.draw_image(self.background, 0, 0)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_y = self.height // 2\n",
        "        self.agent_velocity = 0\n",
        "        self.frame_count = 0\n",
        "        self.pipes = []\n",
        "        gap_y = np.random.randint(30, self.height - 30 - self.pipe_gap)\n",
        "        self.pipes.append((self.width, gap_y))\n",
        "        self.done = False\n",
        "        self.total_reward = 0\n",
        "        self.last_action = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action, render=True):\n",
        "        if self.done:\n",
        "            return self._get_obs(), 0, True, {}\n",
        "\n",
        "        self.last_action = action\n",
        "        self.agent_velocity = self.lift if action == 1 else self.agent_velocity + self.gravity\n",
        "        self.agent_y = max(self.agent_radius, min(self.height - self.agent_radius, self.agent_y + self.agent_velocity))\n",
        "\n",
        "        new_pipes = []\n",
        "        for x, gap_y in self.pipes:\n",
        "            x -= self.pipe_speed\n",
        "            if x + self.pipe_width > 0:\n",
        "                new_pipes.append((x, gap_y))\n",
        "        self.pipes = new_pipes\n",
        "\n",
        "        if len(self.pipes) == 0 or (self.width - self.pipes[-1][0]) > np.random.randint(self.min_pipe_spacing, self.max_pipe_spacing):\n",
        "            gap_y = np.random.randint(30, self.height - 30 - self.pipe_gap)\n",
        "            self.pipes.append((self.width, gap_y))\n",
        "\n",
        "        reward = 1\n",
        "        for x, gap_y in self.pipes:\n",
        "            if (self.agent_x + self.agent_radius > x) and (self.agent_x - self.agent_radius < x + self.pipe_width):\n",
        "                if not (gap_y <= self.agent_y <= gap_y + self.pipe_gap):\n",
        "                    self.done = True\n",
        "                    reward = -100\n",
        "\n",
        "        if self.agent_y <= self.agent_radius or self.agent_y >= self.height - self.agent_radius:\n",
        "            self.done = True\n",
        "            reward = -100\n",
        "\n",
        "        self.frame_count += 1\n",
        "        self.total_reward += reward\n",
        "        if render:\n",
        "            self.render()\n",
        "            import time\n",
        "            time.sleep(1 / self.frame_rate)\n",
        "        return self._get_obs(), reward, self.done, {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        for x, gap_y in self.pipes:\n",
        "            if x + self.pipe_width >= self.agent_x:\n",
        "                gap_center = gap_y + self.pipe_gap / 2\n",
        "                dist_to_pipe = x - self.agent_x\n",
        "                return np.array([self.agent_y, self.agent_velocity, gap_center, dist_to_pipe], dtype=np.float32)\n",
        "        return np.array([self.agent_y, self.agent_velocity, self.height / 2, self.width], dtype=np.float32)\n",
        "\n",
        "    def render(self):\n",
        "        self.pipe_layer.clear()\n",
        "        self.agent_layer.clear()\n",
        "\n",
        "        for x, gap_y in self.pipes:\n",
        "            # Bottom pipe\n",
        "            self.pipe_layer.draw_image(self.pipe, x, gap_y + self.pipe_gap)\n",
        "\n",
        "            # Top pipe (flipped)\n",
        "            self.pipe_layer.save()\n",
        "            self.pipe_layer.translate(x + 20, gap_y)  # 20 = half width of pipe\n",
        "            self.pipe_layer.scale(1, -1)\n",
        "            self.pipe_layer.draw_image(self.pipe, -20, 0)\n",
        "            self.pipe_layer.restore()\n",
        "\n",
        "        # Agent (until we add sprites)\n",
        "\n",
        "        # Choose bird image based on last action\n",
        "        bird_img = self.bird_up if self.last_action == 1 else self.bird_down\n",
        "        self.agent_layer.draw_image(bird_img, self.agent_x - 20, self.agent_y - 20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itv2fcnm6RRt"
      },
      "source": [
        "Let's learn about the standard environment interface and test it with a random agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POTv9YfW6RRu"
      },
      "outputs": [],
      "source": [
        "def random_policy(obs):\n",
        "    return np.random.choice([0, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LNvnF6y6RRu",
        "outputId": "c2da50d9-2389-46a5-c214-3109c81037ab",
        "colab": {
          "referenced_widgets": [
            "e7ea21a72bac45de900fc33830ca0612"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7ea21a72bac45de900fc33830ca0612",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "MultiCanvas(height=300, width=600)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Game over. Total reward: -81\n"
          ]
        }
      ],
      "source": [
        "def evaluate_policy(policy):\n",
        "  env = FlappyMazeCanvasEnv()\n",
        "\n",
        "  episode_steps = 200\n",
        "  total_reward = 0\n",
        "  obs = env.reset()\n",
        "  for step in range(episode_steps):\n",
        "      action = policy(obs)\n",
        "      obs, reward, done, info = env.step(action)\n",
        "      total_reward += reward\n",
        "      if done or step == episode_steps - 1:\n",
        "          print(f\"Game over. Total reward: {total_reward}\")\n",
        "          break\n",
        "\n",
        "evaluate_policy(random_policy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoNtwBkN6RRu"
      },
      "source": [
        "Let's break down our environment interface:\n",
        "## Action Space\n",
        "\n",
        "- **Type**: Discrete(2)\n",
        "- **Values**:\n",
        "  - `0`: Do nothing (bird is pulled down by gravity)\n",
        "  - `1`: Flap (apply upward lift)\n",
        "\n",
        "## Observation Space\n",
        "\n",
        "- **Type**: `np.array` of shape `(4,)`, dtype `float32`\n",
        "- **Observation Values**:\n",
        "  1. `agent_y`: Vertical position of the bird\n",
        "  2. `agent_velocity`: Current vertical velocity\n",
        "  3. `gap_center`: Vertical center of the upcoming pipe gap\n",
        "  4. `dist_to_pipe`: Horizontal distance to the next pipe\n",
        "\n",
        "## Reward\n",
        "\n",
        "- +1 for surviving each frame\n",
        "- -100 for crashing:\n",
        "  - Into a pipe\n",
        "  - Into the top or bottom of the canvas\n",
        "\n",
        "## Done\n",
        "- `done = True` when:\n",
        "  - The bird crashes into a pipe\n",
        "  - The bird hits the floor or ceiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWpwqN9U6RRu"
      },
      "source": [
        "Let's design an expert agent that will solve the game and will provide us with demonstrations for our policy to learn from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEQ53LfJ6RRu"
      },
      "outputs": [],
      "source": [
        "def expert_policy(obs):\n",
        "    agent_y, _, gap_center, _ = obs\n",
        "    return 1 if agent_y > gap_center else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjM2pAHm6RRu",
        "outputId": "2ab84a0a-84c7-4c5a-a4d7-f0fc66d6cb5c",
        "colab": {
          "referenced_widgets": [
            "b9f1f5d7ce7142d29b769bd672e470a9"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9f1f5d7ce7142d29b769bd672e470a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "MultiCanvas(height=300, width=600)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Game over. Total reward: -2\n"
          ]
        }
      ],
      "source": [
        "evaluate_policy(expert_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNoCEzbL6RRu"
      },
      "source": [
        "Let's collect 20 demos to learn our policy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9H_lD_t6RRu",
        "outputId": "04ae331a-530f-424a-dad3-8323524df7f0",
        "colab": {
          "referenced_widgets": [
            "52d99c997cd941398c58cfeac62638ef"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52d99c997cd941398c58cfeac62638ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "MultiCanvas(height=300, width=600)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data = []\n",
        "env = FlappyMazeCanvasEnv()\n",
        "num_episodes = 20\n",
        "for _ in range(num_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = expert_policy(obs)\n",
        "        data.append({\n",
        "            \"action\": int(action),\n",
        "            \"obs\": {\n",
        "                \"agent_y\": float(obs[0]),\n",
        "                \"agent_velocity\": float(obs[1]),\n",
        "                \"gap_center\": float(obs[2]),\n",
        "                \"pipe_dist\": float(obs[3])\n",
        "            }\n",
        "        })\n",
        "        obs, _, done, _ = env.step(action, render=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDAgCUk76RRu"
      },
      "source": [
        "After this detour to setup our environment, let's go back to learning more pytorch essentials.\n",
        "\n",
        "In **supervised learning**, we train models using labeled data â€” typically a dataset of input-output pairs. To work with datasets in PyTorch, we use two core abstractions:\n",
        "\n",
        "### `torch.utils.data.Dataset`\n",
        "\n",
        "The `Dataset` class is a **wrapper for your data**. You define a custom dataset by subclassing `torch.utils.data.Dataset` and implementing two key methods:\n",
        "\n",
        "- `__len__`: returns the number of samples in the dataset.\n",
        "- `__getitem__`: fetches a sample at a given index.\n",
        "\n",
        "This is where you load and preprocess your data entries **one-by-one**.\n",
        "\n",
        "### `torch.utils.data.DataLoader`\n",
        "The `DataLoader` wraps a `Dataset` and helps you iterate through it efficiently. It handles:\n",
        "\n",
        "- *Batching*: groups multiple samples into mini-batches.\n",
        "- *Shuffling*: randomizes sample order each epoch.\n",
        "- *Parallel loading*: uses multiprocessing for faster loading.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD2xTRYx6RRu"
      },
      "source": [
        "Let's define our `FlappyDataset`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpRrxUTD6RRu"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class FlappyDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        obs_dict = entry[\"obs\"]\n",
        "        obs_tensor = torch.tensor([\n",
        "            obs_dict[\"agent_y\"],\n",
        "            obs_dict[\"agent_velocity\"],\n",
        "            obs_dict[\"gap_center\"],\n",
        "            obs_dict[\"pipe_dist\"]\n",
        "        ], dtype=torch.float32)\n",
        "        action_tensor = torch.tensor(entry[\"action\"], dtype=torch.long)\n",
        "        return obs_tensor, action_tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIijut4L6RRv"
      },
      "source": [
        "As in all supervised learning we need to split the dataset in train and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPshJOWs6RRv"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "train_ratio = 0.8\n",
        "\n",
        "random.shuffle(data)\n",
        "\n",
        "split_idx = int(len(data) * train_ratio)\n",
        "train_data = data[:split_idx]\n",
        "val_data = data[split_idx:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb2Hp3Dy6RRv"
      },
      "outputs": [],
      "source": [
        "train_dataset = FlappyDataset(train_data)\n",
        "val_dataset = FlappyDataset(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePJMuHNE6RRv"
      },
      "source": [
        "Now let's wrap our datasets into dataloaders for efficient access to our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcvrJCoc6RRv"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWQnkUGp6RRv"
      },
      "source": [
        "# Define the Policy Network\n",
        "\n",
        "### `nn.Module`\n",
        "`nn.Module` is a useful class that corresponds loosely to an intuitive notion of a parameterized model. It\n",
        "* registers `nn.Parameter`s, which are essentially variable tensors, as attributes\n",
        "* holds the `forward` method, the model's forward pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M42muCmV6RRv"
      },
      "source": [
        "Let's start defining a simple linear model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn12GmAm6RRv"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jypo9wso6RRv"
      },
      "outputs": [],
      "source": [
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(784, 10))\n",
        "        self.b = nn.Parameter(torch.randn(10))\n",
        "\n",
        "    def forward(self, x_b):\n",
        "        return x_b @ self.W + self.b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poFBCmfE6RRv"
      },
      "source": [
        "Let's continue with a Multi-Layer-Perceptron. This consists of a list of linear fully connected layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Wy1rLPe6RRv"
      },
      "outputs": [],
      "source": [
        "class MultiLayerPerceptron(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # ATTENTION: Don't forget to wrap the list of layers around a ModuleList\n",
        "        # OR it won't be detected by pytorch as trainable parameters\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Linear(4, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x_b):\n",
        "        for layer in self.layers:\n",
        "            x_b = layer(x_b)\n",
        "        return x_b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA_DuHAB6RRv"
      },
      "source": [
        "### `nn.Sequential`\n",
        "It's pretty common to have a sequence of layers we want to iteratively apply onto an input. We can replace the `for` loop business in the forward pass by using an `nn.Sequential` container, a very commonly used abstraction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DahBpcY46RRv"
      },
      "outputs": [],
      "source": [
        "class MultiLayerPerceptron(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(4, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmqadHwE6RRv"
      },
      "source": [
        "With this knowledge let's create our learning agent or behavior cloning policy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSYDaGSA6RRv"
      },
      "outputs": [],
      "source": [
        "class BCPolicy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(4, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0um2ZwTV6RRv"
      },
      "outputs": [],
      "source": [
        "policy = BCPolicy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJvU9AY56RRv"
      },
      "source": [
        "Without training it this policy is completly useless right now :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiQN932p6RRv",
        "outputId": "f3758fe9-cf9a-40f6-9c7b-876b9338587a",
        "colab": {
          "referenced_widgets": [
            "7cd4bc8537234bdba7b4e0314245e744"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cd4bc8537234bdba7b4e0314245e744",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "MultiCanvas(height=300, width=600)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode finished with total reward: -76\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the policy\n",
        "env = FlappyMazeCanvasEnv()  # re-creates and re-displays the canvas here\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "step = 0\n",
        "while not done and step < 250:\n",
        "    obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        logits = policy(obs_tensor)\n",
        "        action = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    step += 1\n",
        "\n",
        "print(f\"Episode finished with total reward: {total_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFpPBZbY6RRw"
      },
      "source": [
        "# Time to train the policy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFp0uzDf6RRw"
      },
      "source": [
        "### Defining the loss function\n",
        "\n",
        "Let's start looking into how we would define a simple l1-regression loss:\n",
        "\n",
        "\n",
        "*Revision*:\n",
        "$$\n",
        "\\mathcal{L}_{\\text{L1}}(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "$\\hat{y}$: is the prediction, usually the output of your network\n",
        "\n",
        "$y$: is the target that you want your network to match\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn_w-Wua6RRw"
      },
      "outputs": [],
      "source": [
        "# We can define the loss function manually\n",
        "def l1_loss_manual(predictions, targets):\n",
        "    return torch.mean(torch.abs(predictions - targets))\n",
        "\n",
        "loss_fn_custom = l1_loss_manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lcbqaa-E6RRw"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "# We can use PyTorch library predefined loss functions\n",
        "loss_fn_pytorch = nn.L1Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lN31nGg6RRw",
        "outputId": "d474d27c-40e7-4734-ddb5-b95e41865a76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L1 Loss with custom function: 0.36666664481163025\n",
            "L1 Loss with pytorch function: 0.36666664481163025\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "predictions = torch.tensor([2.5, 0.0, 2.1], requires_grad=True) # this is usually the output of your network\n",
        "targets = torch.tensor([3.0, -0.5, 2.0])\n",
        "\n",
        "loss = loss_fn_custom(predictions, targets)\n",
        "print(\"L1 Loss with custom function:\", loss.item())\n",
        "loss = loss_fn_pytorch(predictions, targets)\n",
        "print(\"L1 Loss with pytorch function:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62mL4Mdz6RRw"
      },
      "source": [
        "For our environment we will use the Cross Entropy Loss since we are dealing with a classification problem -- the agent needs to decide between taking the action 0 (no move) or 1 (go up).\n",
        "\n",
        "*Revision*:\n",
        "Cross-entropy loss is commonly used for **classification problems**, especially when the task involves predicting a **probability distribution over multiple classes**.\n",
        "$$\n",
        "\\mathcal{L}_{CE}(y, \\hat{y}) = - \\sum_{c=0}^{C} I[y == c] \\log(\\hat{y}_c)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB54ZIT-6RRw"
      },
      "outputs": [],
      "source": [
        "# We can define it manually:\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def cross_entropy_loss(logits, targets):\n",
        "    log_probs = F.log_softmax(logits, dim=1)\n",
        "    return -log_probs[range(len(targets)), targets].mean() # Negative log-likelihood\n",
        "\n",
        "loss_fn_custom = cross_entropy_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_FxXBfI6RRw"
      },
      "outputs": [],
      "source": [
        "# We can use the predefined loss in pytorch\n",
        "\n",
        "loss_fn_pytorch = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGRYQH5_6RRw",
        "outputId": "53f2a51c-259f-4e68-cb7a-074c213bd335"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Entropy Loss with custom function: 0.47528108954429626\n",
            "Cross-Entropy Loss with pytorch function: 0.47528108954429626\n"
          ]
        }
      ],
      "source": [
        "# Example logits (not probabilities)\n",
        "logits = torch.tensor([[1.2, 0.5, 2.1]], requires_grad=True)  # shape (1, 3)\n",
        "# Target class index\n",
        "target = torch.tensor([2])  # shape (1,)\n",
        "\n",
        "# Compute the loss\n",
        "loss = loss_fn_custom(logits, target)\n",
        "print(\"Cross-Entropy Loss with custom function:\", loss.item())\n",
        "# Compute the loss\n",
        "loss = loss_fn_pytorch(logits, target)\n",
        "print(\"Cross-Entropy Loss with pytorch function:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FlcuX9L6RRw"
      },
      "source": [
        "##PyTorch Autograd\n",
        "\n",
        "Pytorch is well-known for its automatic differentiation feature. We can call the `backward()` method to ask `PyTorch` to calculate the gradients, which are then stored in the `grad` attribute. Let's dive into some examples!\n",
        "\n",
        "Let's start the basics. For PyTorch to track the gradient we must set the tensor `requires_grad` attribute to `True`. It basically tells PyTorch, hey this is a variable I want to keep track of.\n",
        "\n",
        "Note: this is done automatically for the parameters of the neural networks if you use the correct wrappers as we showed before (`nn.Module`,`nn.ModuleList`, `nn.Parameter`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA6uH6vF6RRw",
        "outputId": "17ef5a8c-54e7-4628-8a63-8557fb836ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# Create an example tensor\n",
        "# requires_grad parameter tells PyTorch to store gradients\n",
        "x = torch.tensor([2.], requires_grad=True)\n",
        "\n",
        "# Print the gradient if it is calculated\n",
        "# Currently None since x is a scalar\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKTzTgWF6RRw",
        "outputId": "d578b6e6-01b1-4eac-8322-9f7af3551a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([12.])\n"
          ]
        }
      ],
      "source": [
        "# Calculating the gradient of y with respect to x\n",
        "y = x * x * 3 # 3x^2\n",
        "y.backward()\n",
        "print(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyICAo5m6RRw"
      },
      "source": [
        "Let's run backprop from a different tensor again to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VFsrIJF6RRw",
        "outputId": "770fb25a-baa1-427a-b4bf-17a150f45e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([24.])\n"
          ]
        }
      ],
      "source": [
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "print(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12 ???!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtYCV-6b6RRw"
      },
      "source": [
        "Why are we getting 24 here? Shouldn't it be 12 too?\n",
        "\n",
        "The reason is `backward` accumulates gradients, which means it keeps adding them up as you keep calling it. For this reason it's important to set the gradients back to 0 at the start of the operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVdli0bX6RRw",
        "outputId": "20d066ea-a8e2-48a5-8488-9636ccc8dbab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([12.])\n"
          ]
        }
      ],
      "source": [
        "x.grad = None\n",
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "# y = x * x * 3\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G36--Wx26RRw",
        "outputId": "7b636e29-228d-443e-b7e9-4d16c27db03a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([12.])\n"
          ]
        }
      ],
      "source": [
        "x.grad = None\n",
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "print(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12 !! Yay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kinCQnjm6RRw"
      },
      "source": [
        "Yay! We get 12 again!\n",
        "\n",
        "The reason why `backward` is implemented to update the sum of the gradients calculated so far is because when we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run `zero_grad()` in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyAzwGfU6RRx"
      },
      "source": [
        "### `torch.optim`\n",
        "`torch.optim` contains implementations of many common optimization algorithms. When initializing an optimizer instance, we need to pass in the model parameters. Now, instead of manually coding the update rule, we call the optimizer's `step` method. The optimizer can also handle resetting parameter gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McXglChE6RRx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtmZKr4C6RRx"
      },
      "outputs": [],
      "source": [
        "model = torch.nn.Linear(10, 2)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftiZIZEb6RRx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKBuPHAe6RRx"
      },
      "source": [
        "## Back to our training loop!\n",
        "\n",
        "We're getting there! Let's define our training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msiFI_qB6RRx",
        "outputId": "5fd23505-8652-47ac-cbe4-88b70b9fb9fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7a497416f390>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Instantiate the necessary components for our training\n",
        "policy = BCPolicy()\n",
        "optimizer = torch.optim.SGD(policy.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# The train and val dataloaders have been defined before\n",
        "train_loader\n",
        "val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxPOvwWy6RRx",
        "outputId": "b5cdaedc-fa40-49f5-d75f-1c05653325b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 0.8358, Val Accuracy = 96.82%\n",
            "Epoch 2: Loss = 0.1229, Val Accuracy = 98.09%\n",
            "Epoch 3: Loss = 0.0786, Val Accuracy = 89.57%\n",
            "Epoch 4: Loss = 0.0605, Val Accuracy = 90.67%\n",
            "Epoch 5: Loss = 0.0616, Val Accuracy = 98.39%\n"
          ]
        }
      ],
      "source": [
        "# Remember to use the GPU\n",
        "policy.to('cuda')\n",
        "\n",
        "num_epochs = 5  # Short and sweet for tutorial\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x = x.to('cuda')\n",
        "        y = y.to('cuda')\n",
        "        logits = policy(x)\n",
        "        loss = loss_fn(logits, y)\n",
        "        optimizer.zero_grad() # reset the computed gradients to 0\n",
        "        loss.backward() # compute the gradients\n",
        "        optimizer.step() # take one step using the computed gradients and optimizer\n",
        "        total_loss += loss.item() # track your loss\n",
        "\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x = x.to('cuda')\n",
        "            y = y.to('cuda')\n",
        "            logits = policy(x)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item() # compute the accuracy\n",
        "            total += len(y)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}, Val Accuracy = {val_acc:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZBHeYjO6RRx"
      },
      "source": [
        "# Time to see how our policy does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTK_HZIm6RRx",
        "outputId": "23288dbe-da30-4779-9a6f-165efa83750f",
        "colab": {
          "referenced_widgets": [
            "2dc4c1cac1964f56b0051c7f67c28dca"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2dc4c1cac1964f56b0051c7f67c28dca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "MultiCanvas(height=300, width=600)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode finished with total reward: 250\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the policy\n",
        "env = FlappyMazeCanvasEnv()  # re-creates and re-displays the canvas here\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "step = 0\n",
        "while not done and step < 250:\n",
        "    obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        logits = policy(obs_tensor)\n",
        "        action = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    step += 1\n",
        "\n",
        "print(f\"Episode finished with total reward: {total_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfzNJIb66RRx"
      },
      "source": [
        "### Yay! It worked great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z3Gsrsi6RRx"
      },
      "source": [
        "###Saving/Loading pre-trained weights\n",
        "Sometimes multiple people may have to use the same network. It can take a lot of time for everyone to train their network from scratch. It is environmentally also disastrous as the carbon footprint of training large networks is very huge.\n",
        "\n",
        "It would be better if we could train and save weights for our network, and reuse them later if we need to. PyTorch gives us ways to do this easily!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giXIuazC6RRx"
      },
      "outputs": [],
      "source": [
        "# Save your policy weights\n",
        "torch.save(policy.state_dict(), \"network.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8da-OhEu6RRx",
        "outputId": "d2c0ca65-e08b-4397-9274-217e01282cea",
        "colab": {
          "referenced_widgets": [
            "fd421313e8b345328c9951ba73a2db22"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd421313e8b345328c9951ba73a2db22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "MultiCanvas(height=300, width=600)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized Network: Episode finished with total reward: -83\n",
            "Loaded Network: Episode finished with total reward: 250\n"
          ]
        }
      ],
      "source": [
        "# Reinitialize the policy\n",
        "policy_new = BCPolicy()\n",
        "env = FlappyMazeCanvasEnv()\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "step = 0\n",
        "while not done and step < 250:\n",
        "    obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        logits = policy_new(obs_tensor)\n",
        "        action = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    step += 1\n",
        "\n",
        "print(f\"Initialized Network: Episode finished with total reward: {total_reward}\")\n",
        "\n",
        "policy_new.load_state_dict(torch.load(\"network.pt\"))\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "step = 0\n",
        "while not done and step < 250:\n",
        "    obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        logits = policy_new(obs_tensor)\n",
        "        action = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    step += 1\n",
        "\n",
        "print(f\"Loaded Network: Episode finished with total reward: {total_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrMhvX_a6RRx"
      },
      "source": [
        "# BONUS: Learning the details of Autograd through Physics Informed Neural Networks!\n",
        "\n",
        "This example holds dear in my heart because this was how I was finally able to properly understand PyTorch's autograd, and I hope it can do the same to you (but it's okay if it doesn't become obvious right away, as all learning it takes time).\n",
        "\n",
        "Letâ€™s solve the next differential equation with our dear friend the neural networks:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = -y, \\quad y(0) = 1\n",
        "$$\n",
        "\n",
        "The true solution is $( y(x) = e^{-x} )$.\n",
        "\n",
        "We'll teach a neural network to **learn this function**, by making sure its derivative matches the ODE.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMkIYGwh6RRx"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(1, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dkhrZMh6RRx"
      },
      "source": [
        "Before we learned to get the gradients by computing them through `backward` however pytorch also exposes the `torch.autograd.grad` function that allows us to pass the function and variable that we want want to compute the gradient of and with respect to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsInVfMs6RRx"
      },
      "outputs": [],
      "source": [
        "model = SimpleNN()\n",
        "x = torch.tensor([[0.5]], requires_grad=True)\n",
        "y = model(x)\n",
        "\n",
        "# Compute dy/dx\n",
        "dy_dx = torch.autograd.grad(\n",
        "    y, x,\n",
        "    grad_outputs=torch.ones_like(y),\n",
        "    create_graph=True\n",
        ")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb3_WXzX6RRx"
      },
      "source": [
        "Let's define the ODE loss by simply setting the constraints defined by this ODE. On one side:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} + y = 0\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "y(0) = 1\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbrPIN4L6RRy"
      },
      "outputs": [],
      "source": [
        "def ode_loss(x):\n",
        "    x.requires_grad_(True)\n",
        "    y = model(x)\n",
        "    dy_dx = torch.autograd.grad(y, x, torch.ones_like(y), create_graph=True)[0]\n",
        "    residual = dy_dx + y # first term\n",
        "    bc = (model(torch.tensor([[0.0]])) - 1.0)**2 # second term\n",
        "    return torch.mean(residual**2) + bc # we combine both terms by adding them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px2Rr0C_6RRy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C77GGdE6RRy"
      },
      "source": [
        "Now let's train the model by evaluating it on a bunch of points between [0,1] and make it satisfy our ODE equation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukzmMD-R6RRy",
        "outputId": "3e92580a-16c8-4808-affe-2f1099808a11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.5126084089279175\n",
            "Epoch 100, Loss: 0.0005569627974182367\n",
            "Epoch 200, Loss: 0.0002499284746591002\n",
            "Epoch 300, Loss: 0.00010241399286314845\n",
            "Epoch 400, Loss: 1.3791658602713142e-05\n",
            "Epoch 500, Loss: 3.5041593946516514e-05\n",
            "Epoch 600, Loss: 1.578538285684772e-05\n",
            "Epoch 700, Loss: 1.2067529496562202e-05\n",
            "Epoch 800, Loss: 1.6937650798354298e-05\n",
            "Epoch 900, Loss: 7.352070497290697e-06\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    x = torch.rand(32, 1)\n",
        "    loss = ode_loss(x)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sc0hpB46RRy",
        "outputId": "da286021-57db-461c-abcf-fa41b2912bb3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_test = torch.linspace(0, 1, 100).reshape(-1, 1)\n",
        "y_pred = model(x_test).detach().numpy()\n",
        "y_true = torch.exp(-x_test).numpy()\n",
        "\n",
        "plt.plot(x_test, y_pred, label=\"PINN Prediction\")\n",
        "plt.plot(x_test, y_true, '--', label=\"True Solution\")\n",
        "plt.legend()\n",
        "plt.title(\"Using Autograd to Solve dy/dx = -y\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pvj3wLb6RRy"
      },
      "source": [
        "# More PyTorch Features: A Whirlwind Tour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxeoH48r6RRy"
      },
      "source": [
        "### CNN\n",
        "2D convolutional layers expect an input of shape $(B, C, H, W)$ or `(batch, channels, height, width)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0GZTZKn6RRy"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=20, kernel_size=(5, 5), stride=1, padding=0),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5), stride=1, padding=0),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(800, 500),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(500, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_b):\n",
        "        return self.layers(x_b.view((-1, 1, 28, 28)))\n",
        "\n",
        "model = CNN().to('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r4kOAuM6RRy"
      },
      "source": [
        "### LSTM\n",
        "When constructed using the `batch_first=True` argument, `nn.LSTM`s expect inputs of shape $(B, L, D)$ or `(batch, sequence length, element size)`. We'll interpret the time sequence images as sequence elements, and compute the logits from the last LSTM output (i.e. wait for the LSTM to see the entire image)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HYiHMl_6RRy"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.LSTM = nn.LSTM(input_size=28, hidden_size=64, num_layers=1, batch_first=True)\n",
        "        self.linear = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x_b):\n",
        "        out, _ = self.LSTM(x_b.view((-1, 28, 28)))  # out contains outputs at each iteration over the sequence\n",
        "        return self.linear(out[:, -1, :])   # only the out at the last iteration has seen the entire image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5nlNFkI6RRy"
      },
      "source": [
        "### `nn.Embedding`\n",
        "Oftentimes we will have categorical data with too many categories to comfortably convert into one-hot encodings. Examples of this include tokens in NLP and $(x,y)$ positions on a grid. One common way to process this data is to use an `nn.Embedding`, or a table of dense vectors that can be indexed by the categorical data. The dense vectors can be optimized. `nn.Embedding` supports advanced indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcz8XVwO6RRy",
        "outputId": "25d90474-7d2f-47ad-fda1-fb6cb5424e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64])\n",
            "True\n",
            "torch.Size([2, 4])\n",
            "torch.Size([2, 4, 64])\n"
          ]
        }
      ],
      "source": [
        "embedding = nn.Embedding(num_embeddings=1000, embedding_dim=64)\n",
        "print(embedding(torch.tensor(42)).shape)\n",
        "print(embedding(torch.tensor(42)).requires_grad)\n",
        "indices = torch.tensor([\n",
        "    [0, 1, 42, 999],\n",
        "    [1, 1, 2, 3]\n",
        "])\n",
        "print(indices.shape)\n",
        "print(embedding(indices).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um-Yvpob6RRy"
      },
      "source": [
        "If you're interested in NLP, a nice use of `nn.Embedding` and recurrent layers (`nn.GRU`) for natural language translation can be found in [this tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c9GOhhu6RRy"
      },
      "source": [
        "### `torch.distributions`\n",
        "Aside from conveniently providing commonly used methods (e.g. density/mass evaluation), one of the main features of this module is the use of the reparameterization trick to facilitate backpropagation through random samples to the underlying distribution parameters (and beyond)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXdaUUk36RRy",
        "outputId": "3588376e-515b-4743-c030-18ed2642ac69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0.9733, -0.3250,  0.4241, -0.9791,  1.1398], grad_fn=<AddBackward0>)\n",
            "tensor(-1.3963)\n"
          ]
        }
      ],
      "source": [
        "from torch import distributions\n",
        "loc = torch.tensor(0., requires_grad=True)\n",
        "scale = torch.tensor(1., requires_grad=True)\n",
        "p = distributions.Normal(loc, scale)\n",
        "x = p.rsample(torch.Size([5]))\n",
        "print(x)    # grad_fn comes from reparameterization trick\n",
        "y = -torch.mean(x**2)\n",
        "y.backward()\n",
        "print(scale.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVk4XFSl6RRz"
      },
      "source": [
        "If we don't use `rsample`, we can't take gradients through the sampling step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywZqzct66RRz",
        "outputId": "7005658e-09d5-432f-f282-50b9677e201a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-1.1755,  0.5557, -0.1999,  0.5991, -0.4827])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
            "\u001b[0;32m<ipython-input-60-694ee59c3544>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
            "\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
            "\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    625\u001b[0m             )\n",
            "\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n",
            "\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    628\u001b[0m         )\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
            "\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n",
            "\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "x = p.sample(torch.Size([5]))\n",
        "print(x)\n",
        "y = -torch.sum(x**2)\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To0FY9M76RRz"
      },
      "source": [
        "# Debugging, Documentation, and Getting Help\n",
        "Your assignments won't be in Jupyter notebooks, but in Python scripts. This means you won't have the interactivity of cell-based execution, but to the rescue comes `pdb`, the Python Debugger. Use it to\n",
        "\n",
        "*   set breakpoints in the code to interactively inspect program elements: `import pdb; pdb.set_trace()`\n",
        "*   automatically start a debugging session when an exception is thrown: `python -m pdb -c continue main.py`\n",
        "\n",
        "Both will be super, super useful.\n",
        "\n",
        "When something about PyTorch is confusing you, the first place to look is the [PyTorch documentation](https://https://pytorch.org/docs/stable/index.html). (The author of this tutorial consulted the documentation no fewer than 20 times when making it.) This is often the last place you need to look, but sometimes the [PyTorch forums](https://https://discuss.pytorch.org/) can be helpful, too. (Use the search tool!) And of course, you're always welcome to post on Ed or seek out one of your friendly neighborhood CAs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ0In-fU6RRz"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}